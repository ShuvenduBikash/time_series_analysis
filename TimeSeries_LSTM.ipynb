{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TimeSeries_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1J15Vh_1Jih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "7c546ea1-a45e-4686-ff83-92e8672f686c"
      },
      "source": [
        "!pip install tf-nightly-2.0-preview\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/c6abb085289f2d03b779c40555dc6228df0a17e57b5b1cccc42f6cd39aaf/tf_nightly_2.0_preview-2.0.0.dev20190910-cp36-cp36m-manylinux2010_x86_64.whl (90.6MB)\n",
            "\u001b[K     |████████████████████████████████| 90.6MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.33.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.16.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.7.1)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/b7/b08d85548da34bf0c6e102fdbb9f88b6319624adbb218bedea5f6075da67/tensorflow_estimator_2.0_preview-1.14.0.dev2019091001-py2.py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Collecting tb-nightly<1.16.0a0,>=1.15.0a0 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/58/d1faaae4b1cd484b72e066ba8bac4655603bce560e9f15a1b825ebdc8acd/tb_nightly-1.15.0a20190910-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-2.0-preview) (0.15.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-2.0-preview) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-2.0-preview) (2.8.0)\n",
            "Installing collected packages: tensorflow-estimator-2.0-preview, tb-nightly, tf-nightly-2.0-preview\n",
            "Successfully installed tb-nightly-1.15.0a20190910 tensorflow-estimator-2.0-preview-1.14.0.dev2019091001 tf-nightly-2.0-preview-2.0.0.dev20190910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOjujz601HcS",
        "colab_type": "code",
        "outputId": "bd7efec6-d123-44d6-f8d7-1e9a79565021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-dev20190910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zswl7jRtGzkk",
        "colab": {}
      },
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)\n",
        "\n",
        "def trend(time, slope=0):\n",
        "    return slope * time\n",
        "\n",
        "def seasonal_pattern(season_time):\n",
        "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
        "    return np.where(season_time < 0.4,\n",
        "                    np.cos(season_time * 2 * np.pi),\n",
        "                    1 / np.exp(3 * season_time))\n",
        "\n",
        "def seasonality(time, period, amplitude=1, phase=0):\n",
        "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
        "    season_time = ((time + phase) % period) / period\n",
        "    return amplitude * seasonal_pattern(season_time)\n",
        "\n",
        "def noise(time, noise_level=1, seed=None):\n",
        "    rnd = np.random.RandomState(seed)\n",
        "    return rnd.randn(len(time)) * noise_level\n",
        "\n",
        "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
        "baseline = 10\n",
        "series = trend(time, 0.1)  \n",
        "baseline = 10\n",
        "amplitude = 40\n",
        "slope = 0.05\n",
        "noise_level = 5\n",
        "\n",
        "# Create the series\n",
        "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
        "# Update with noise\n",
        "series += noise(time, noise_level, seed=42)\n",
        "\n",
        "split_time = 1000\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]\n",
        "\n",
        "window_size = 20\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sTTIOCbyShY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
        "  dataset = dataset.batch(batch_size).prefetch(1)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1Hl39rklkLm",
        "colab_type": "code",
        "outputId": "f106aeb8-bcec-4645-bc05-df0c996fa5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])\n",
        "history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/100\n",
            "31/31 [==============================] - 6s 194ms/step - loss: 21.4949 - mae: 22.0115\n",
            "Epoch 2/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 21.2168 - mae: 21.6440\n",
            "Epoch 3/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 20.7111 - mae: 21.2278\n",
            "Epoch 4/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 20.1562 - mae: 20.7739\n",
            "Epoch 5/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 19.7903 - mae: 20.2843\n",
            "Epoch 6/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 19.2615 - mae: 19.6776\n",
            "Epoch 7/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 18.2681 - mae: 18.7308\n",
            "Epoch 8/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 17.4057 - mae: 17.9757\n",
            "Epoch 9/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 17.2263 - mae: 17.6581\n",
            "Epoch 10/100\n",
            "31/31 [==============================] - 1s 35ms/step - loss: 16.8978 - mae: 17.3457\n",
            "Epoch 11/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 16.6165 - mae: 17.0459\n",
            "Epoch 12/100\n",
            "31/31 [==============================] - 1s 34ms/step - loss: 16.2132 - mae: 16.7529\n",
            "Epoch 13/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 15.9664 - mae: 16.4573\n",
            "Epoch 14/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 15.6962 - mae: 16.1587\n",
            "Epoch 15/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 15.3739 - mae: 15.8571\n",
            "Epoch 16/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 15.1133 - mae: 15.5765\n",
            "Epoch 17/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 14.7837 - mae: 15.2950\n",
            "Epoch 18/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 14.4760 - mae: 15.0164\n",
            "Epoch 19/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 14.3366 - mae: 14.7612\n",
            "Epoch 20/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 14.0852 - mae: 14.5354\n",
            "Epoch 21/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 13.8134 - mae: 14.3081\n",
            "Epoch 22/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 13.6393 - mae: 14.1062\n",
            "Epoch 23/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 13.3826 - mae: 13.8858\n",
            "Epoch 24/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 13.1954 - mae: 13.7037\n",
            "Epoch 25/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 13.0163 - mae: 13.4526\n",
            "Epoch 26/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 12.7585 - mae: 13.2327\n",
            "Epoch 27/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 12.5284 - mae: 12.9716\n",
            "Epoch 28/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 12.2087 - mae: 12.7095\n",
            "Epoch 29/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 12.3941 - mae: 12.8460\n",
            "Epoch 30/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 11.8443 - mae: 12.3121\n",
            "Epoch 31/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 11.3303 - mae: 11.8148\n",
            "Epoch 32/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 10.9481 - mae: 11.4830\n",
            "Epoch 33/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 10.6266 - mae: 11.1361\n",
            "Epoch 34/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 10.7815 - mae: 11.3007\n",
            "Epoch 35/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 10.9719 - mae: 11.4606\n",
            "Epoch 36/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 10.4490 - mae: 10.9636\n",
            "Epoch 37/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 9.9899 - mae: 10.4892\n",
            "Epoch 38/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 9.5782 - mae: 10.0283\n",
            "Epoch 39/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 9.0664 - mae: 9.5461\n",
            "Epoch 40/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 8.6547 - mae: 9.1358\n",
            "Epoch 41/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 8.2887 - mae: 8.7356\n",
            "Epoch 42/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 7.9123 - mae: 8.4081\n",
            "Epoch 43/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 7.6793 - mae: 8.1137\n",
            "Epoch 44/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 7.3547 - mae: 7.8033\n",
            "Epoch 45/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 7.1047 - mae: 7.5770\n",
            "Epoch 46/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 6.9459 - mae: 7.3933\n",
            "Epoch 47/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 6.5524 - mae: 7.0771\n",
            "Epoch 48/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 6.4136 - mae: 6.9074\n",
            "Epoch 49/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 6.3931 - mae: 6.8757\n",
            "Epoch 50/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 6.0520 - mae: 6.5164\n",
            "Epoch 51/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 5.8715 - mae: 6.3634\n",
            "Epoch 52/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 5.7791 - mae: 6.2421\n",
            "Epoch 53/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 5.6699 - mae: 6.1419\n",
            "Epoch 54/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 5.5072 - mae: 5.9468\n",
            "Epoch 55/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 5.4385 - mae: 5.9006\n",
            "Epoch 56/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.2939 - mae: 5.7471\n",
            "Epoch 57/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 5.3199 - mae: 5.7881\n",
            "Epoch 58/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.1544 - mae: 5.6591\n",
            "Epoch 59/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.3091 - mae: 5.7545\n",
            "Epoch 60/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 5.0288 - mae: 5.5567\n",
            "Epoch 61/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 4.9551 - mae: 5.4250\n",
            "Epoch 62/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.1300 - mae: 5.5808\n",
            "Epoch 63/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 5.2117 - mae: 5.6630\n",
            "Epoch 64/100\n",
            "31/31 [==============================] - 1s 44ms/step - loss: 5.0575 - mae: 5.5312\n",
            "Epoch 65/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 4.9261 - mae: 5.3900\n",
            "Epoch 66/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 5.3047 - mae: 5.7637\n",
            "Epoch 67/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.2755 - mae: 5.7446\n",
            "Epoch 68/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.3003 - mae: 5.7234\n",
            "Epoch 69/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.3746 - mae: 5.8346\n",
            "Epoch 70/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 6.1354 - mae: 6.6386\n",
            "Epoch 71/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 4.9121 - mae: 5.4135\n",
            "Epoch 72/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 5.6100 - mae: 6.0636\n",
            "Epoch 73/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.3411 - mae: 5.8208\n",
            "Epoch 74/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 6.6828 - mae: 7.1966\n",
            "Epoch 75/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.7112 - mae: 6.1957\n",
            "Epoch 76/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.1434 - mae: 5.6158\n",
            "Epoch 77/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.7378 - mae: 6.2385\n",
            "Epoch 78/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 7.2295 - mae: 7.6426\n",
            "Epoch 79/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.0654 - mae: 5.5955\n",
            "Epoch 80/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 6.4766 - mae: 7.0136\n",
            "Epoch 81/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 6.6777 - mae: 7.1079\n",
            "Epoch 82/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 6.6996 - mae: 7.2374\n",
            "Epoch 83/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 6.3449 - mae: 6.8347\n",
            "Epoch 84/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 5.8157 - mae: 6.3277\n",
            "Epoch 85/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 7.0067 - mae: 7.6007\n",
            "Epoch 86/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 7.4649 - mae: 7.9559\n",
            "Epoch 87/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 8.0525 - mae: 8.5456\n",
            "Epoch 88/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 7.0581 - mae: 7.4773\n",
            "Epoch 89/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 6.4190 - mae: 6.9714\n",
            "Epoch 90/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 7.9179 - mae: 8.3901\n",
            "Epoch 91/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 5.9651 - mae: 6.4224\n",
            "Epoch 92/100\n",
            "31/31 [==============================] - 1s 39ms/step - loss: 7.6870 - mae: 8.2557\n",
            "Epoch 93/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 10.2010 - mae: 10.6407\n",
            "Epoch 94/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 9.8401 - mae: 10.2520\n",
            "Epoch 95/100\n",
            "31/31 [==============================] - 1s 36ms/step - loss: 8.0061 - mae: 8.6051\n",
            "Epoch 96/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 10.0772 - mae: 10.7728\n",
            "Epoch 97/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 12.3809 - mae: 12.8968\n",
            "Epoch 98/100\n",
            "31/31 [==============================] - 1s 38ms/step - loss: 11.2224 - mae: 11.6217\n",
            "Epoch 99/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 14.2771 - mae: 14.6141\n",
            "Epoch 100/100\n",
            "31/31 [==============================] - 1s 37ms/step - loss: 10.7625 - mae: 11.1881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkBsrsXMzoWR",
        "colab_type": "code",
        "outputId": "f7b61901-881c-4e76-ce66-be45652c71e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
        "plt.axis([1e-8, 1e-4, 0, 30])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1e-08, 0.0001, 0, 30]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4VdW9//H3N/NAIBACBMI8zwIR\nEBFQcSwqKFLFOlypWIdqrb23tr292uHWWqfeWlEUBxzqUMXZKmqReQqCzFMChEBIAoSQEDKv3x+J\n/JAp0zk5Sfbn9Tx5yNln77O/WZznc/bZe621zTmHiIh4Q1CgCxARkfqj0BcR8RCFvoiIhyj0RUQ8\nRKEvIuIhCn0REQ+pMvTNLMLMVpjZt2a2wcx+V7m8q5ktN7PtZvaWmYX5v1wREamL6hzpFwEXOOcG\nA2cBl5rZSOAR4EnnXA8gB5jmvzJFRMQXqgx9VyG/8mFo5Y8DLgDeqVw+G5jolwpFRMRnqnVO38yC\nzWwNkAV8AaQAh5xzpZWrpAMd/FOiiIj4Skh1VnLOlQFnmVks8B7Qp7o7MLPpwHSA6OjoYX36VHtT\nEREBVq1atd85F++L16pW6H/HOXfIzOYB5wCxZhZSebSfCOw5zTbPAc8BJCUlueTk5DqWLCLiLWa2\ny1evVZ3eO/GVR/iYWSRwEbAJmAdMrlztZuADXxUlIiL+UZ0j/QRgtpkFU/Eh8bZz7mMz2wi8aWZ/\nBFYDL/ixThER8YEqQ985txYYcorlqcBwfxQlIiL+oRG5IiIeotAXEfEQhb6IiIco9EVEPEShLyLi\nIQp9EREPUeiLiHiIQl9ExEMU+iIiHqLQFxHxEIW+iIiHKPRFRDxEoS8i4iEKfRERD1Hoi4h4iEJf\nRMRDFPoiIh6i0BcR8RCFvoiIhyj0RUQ8RKEvIuIhCn0REQ9R6IuIeIhCX0TEQxT6IiIeotAXEfEQ\nhb6IiIdUGfpm1tHM5pnZRjPbYGb3Vi5/yMz2mNmayp/L/V+uiIjURUg11ikF7nfOfWNmMcAqM/ui\n8rknnXOP+a88ERHxpSpD3zmXAWRU/p5nZpuADv4uTEREfK9G5/TNrAswBFheuehuM1trZi+aWUsf\n1yYiIj5W7dA3s2bAu8DPnHOHgWeA7sBZVHwTePw02003s2QzS87OzvZBySIiUlvVCn0zC6Ui8F93\nzs0BcM5lOufKnHPlwPPA8FNt65x7zjmX5JxLio+P91XdIiJSC9XpvWPAC8Am59wTxy1POG61ScB6\n35cnIiK+VJ3eO+cCNwLrzGxN5bJfA9eb2VmAA3YCt/ulQhER8Znq9N5ZBNgpnvrU9+WIiIg/aUSu\niIiHKPRFRDxEoS8i4iEKfRERD1Hoi4h4iEJfRMRDFPoiIh6i0BcR8RCFvoiIhyj0RUQ8RKEvIuIh\nCn0REQ9R6IuIeIhCX0TEQxT6IiIeotAXEfEQhb6IiIco9EVEPEShLyLiIfUa+pmHCykqLavPXYqI\nyHHqNfSz8oq4/P8WsmLHwfrcrYiIVKrX0O8SF01hSTlTZi7lV3PWkXu0pD53LyLiefUa+jERIcy9\nbww/Ht2Vt1amcfGT81mSsr8+SxAR8bR6v5AbHR7Cf0/ox/t3nUt0eAg3zFrOo59vpqSsvL5LERHx\nnID13hmUGMvHPx3NlGEdeXpeCtc+u5S0AwWBKkdExBMC2mUzKiyERyYP4u9Th5CSnc8P/raQz9bv\nC2RJIiJNWoPopz9hUHv+de95dGvTjJ+8topHPttMqU73iIj4XIMIfYDEllG8fftIpo7oxDNfp3Dz\nSys4kF8U6LJERJqUKkPfzDqa2Twz22hmG8zs3srlrczsCzPbVvlvy7oWEx4SzJ8mDeTRyYNI3pnD\nhKcWsX5Pbl1fVkREKlXnSL8UuN851w8YCdxlZv2AB4CvnHM9ga8qH/vEtUkdefeOUQSZce2zS5m7\nQef5RUR8ocrQd85lOOe+qfw9D9gEdACuAmZXrjYbmOjLwgZ0aMF7d42iV9tm3P7aKmYtTMU558td\niIh4To3O6ZtZF2AIsBxo65zLqHxqH9D2NNtMN7NkM0vOzs6uUXFtYiJ4c/o5XNq/HX/8ZBP//f56\nXeAVEamDaoe+mTUD3gV+5pw7fPxzruIQ/JSH4c6555xzSc65pPj4+BoXGBkWzNNTh3LHuO68vjyN\ne99aQ1m5jvhFRGojpDormVkoFYH/unNuTuXiTDNLcM5lmFkCkOWvIoOCjF9e2oeWUaH86dPNRIQE\n8+jkQQQFmb92KSLSJFUZ+mZmwAvAJufcE8c99SFwM/Dnyn8/8EuFx5k+pjsFxWX89cttRIUF8/ur\n+lNRnoiIVEd1jvTPBW4E1pnZmsplv6Yi7N82s2nALmCKf0r8vnsv7MnR4jJmLkglKiyYBy7ro+AX\nEammKkPfObcIOF2qXujbcqpmZjxwWR+OllQEf+tm4dw2plt9lyEi0ig1mBG5NWFmPHRFfy7u15bH\n5m7RRG0iItXUKEMfKi7u/v6qAYQEGQ9+uF59+EVEqqHRhj5AuxYR3HdRL+ZtyWbuxsxAlyMi0uA1\n6tAHuHlUF/q0i+F3H26goLg00OWIiDRojT70Q4OD+MPEAezNLeRvX20PdDkiIg1aow99gLO7tGLy\nsERmLUxlW2ZeoMsREWmwmkToA/zqsj5Eh4fwqznrND+PiMhpNJnQj2sWzu+u7E/yrhwem7s10OWI\niDRITSb0ASYO6cDUEZ14dn4KX6o3j4jISZpU6AP8z4R+DOjQnJ+/vYbdBzVoS0TkeE0u9CNCg5kx\ndRgOuOP1VRSWlAW6JBGRBqPJhT5Ap7gonphyFuv3HOa376+nuFQXdkVEoImGPsBF/dpy57ju/HNV\nOuOfmM/Ha/dqqgYR8bwmG/oA/3lJb2bfOpyosGDu/sdqJj69mOWpBwJdlohIwDTp0DczxvaK55N7\nzuMvkweRebiIHz63jGkvr2TLPg3iEhHvsfo85ZGUlOSSk5PrbX8nOlpcxouLd/Ds/BSOFJVyzdBE\n7ruoF+1jIwNWk4hIVcxslXMuySev5aXQ/07OkWKenredV5buwgxuOqczd4zrQavosECXJiJyEoW+\nj6TnFPDkF9t4b3U6kaHBTBvdlR+P6UbziNBAlyYicoxC38e2Z+XxxBdb+XTdPlpEhnL72G7cMqoL\nUWHVuYWwiIh/KfT9ZP2eXB6fu4V5W7KJiw7jjnHd+dHIzkSEBge6NBHxMIW+n63alcMTX2xh8fYD\ntIkJ5+4LevDDszsSHqLwF5H6p9CvJ8tSD/D43C2s3JlDh9hI7rmwB1cPTSQ0uEn3dBWRBkahX4+c\ncyzctp/H527h2/RcOsdF8bPxPblycAeCgyzQ5YmIB/gy9HXIWgUzY0yveN6/61xm3ZREVFgI9731\nLZf8dQGfrsugvFxTO4hI46HQryYzY3y/tnzy09E8PXUoAHe+/g0TnlrElxszNa+PiDQKCv0aCgoy\nfjAogc9/NoYnfziYI8Wl/PiVZK5+ZglLUvYHujwRkTNS6NdScJAxaUgiX/58LA9fPZB9uYVMfX45\nN8xaxuq0nECXJyJySlWGvpm9aGZZZrb+uGUPmdkeM1tT+XO5f8tsuEKDg7h+eCfm/WIcv53Qj80Z\neUyasYRbXlrBql0KfxFpWKrsvWNmY4B84BXn3IDKZQ8B+c65x2qys8bYe6emjhSV8vKSnbywaAcH\njxQzqnscP72gJyO7tcJMvX1EpObqtfeOc24BcNAXO/OC6PAQ7jq/B4t+eT6/ubwvWzPzuf75ZUyZ\nuZSF27J1wVdEAqou5/TvNrO1lad/Wp5uJTObbmbJZpacnZ1dh901LlFhIdw2phuLfnk+D13Rj90H\nj3LjCyuYNGMJ/96s3j4iEhjVGpxlZl2Aj487vdMW2A844A9AgnPu1qpexwund06nqLSMd1alM2Ne\nCnsOHaV/++bcdX4PLu3fjiAN8hKRMwj44CznXKZzrsw5Vw48Dwz3RTFNWXhIMDeM6MzX/zmOv1wz\niILiMu58/RsuenI+765Kp6RMN28XEf+rVeibWcJxDycB60+3rnxfaHAQU87uyJc/H8tT1w8hNDiI\n+//5LeMe/ZqZ81PILSgJdIki0oRVp/fOG8A4oDWQCTxY+fgsKk7v7ARud85lVLUzL5/eOR3nHF9t\nymLWolSWpR4kMjSYa4Z14JZRXejRJibQ5YlIA6AJ15qojXsP89LiHXzw7V6KS8sZ3qUV1w3vyOUD\nEzSnv4iHKfSbuP35RfwzOZ23Vqax80ABzSNCmDSkA9cN70TfhOaBLk9E6plC3yPKyx3LdhzgzRW7\n+Wz9PorLyjmrYyxTh3diwuAE3c5RxCMU+h6Uc6SYOav38MaKNLZn5dMsPITLBrTjqrM6cE73OM3t\nL9KEKfQ9zDlH8q4c3lyxm8837CO/qJT4mHAmDErg6iGJDOjQXNM9iDQxCn0BoLCkjH9vzuKDNXuY\ntzmb4rJy+rSLYfKwRCYO6UDrZuGBLlFEfEChLyfJLSjho7V7eWdVOmt2HyIkyDivZ2su6d+O8f3a\n6gNApBFT6MsZbcvM451V6XyyLoP0nKMEGSR1bsXlA9txzbBEYiJCA12iiNSAQl+qxTnHxozDzN2Q\nyecb9rF5Xx7RYcFcm9SRm0d1oWvr6ECXKCLVoNCXWlmXnstLi3fw0dq9lJY7zu/dhh+e3ZEL+rQh\nNFg3URNpqBT6UidZeYW8viyNN1akkZVXRFx0GJOGdODapI70bqepH0QaGoW++ERpWTkLtmXzz+R0\nvtyUSUmZY2S3Vvx4dDcu6NNGUz6LNBAKffG5g0eKeWfVbl5evJO9uYV0ax3NraO7cs3QRCLDNO+P\nSCAp9MVvSsrK+XRdBrMW7mDdnlxaRYdx48jO3HROZ+KO6/ZZXu7Ynp1PVFgwiS2jAlixSNOn0Be/\nc86xYsdBnluQylebswgPCeLapEQSW0aRvPMgybtyOFRQQlhwEE9NHcIl/dsFumSRJsuXoa8Zu+SU\nzIwR3eIY0S2ObZl5PL8wlbdXplNcVk631tFc0q8dw7q05I0Vadzx2ir+fM0gpiR1DHTZIlIFHelL\nteUcKabMue+N7i0oLuX2V1excNt+fnN5X24b0y2AFYo0TQG/R654U8vosJOmc4gKC+GFm8/mB4MS\n+N9PN/HIZ5upzwMJEakZnd6ROgsLCeJv1w0hNjKUZ75OIePQUR6ZPIjwEPX6EWloFPriE8FBxh8n\nDqB9bCSPfr6FvYcKmXnjMFpGhwW6NBE5jk7viM+YGXed34Onrh/CmvRDTJqxmB37jwS6LBE5jkJf\nfO6Kwe1547YRHC4sZdKMxbyzKp3ycp3nF2kIFPriF8M6t+K9O0fRJS6aX/zzWybNWMyqXTmBLkvE\n8xT64jed46KZc8convzhYPYdLuSaZ5Zw75urycg9Wu3XyMorZPfBAj9WKeItupArfhUUZEwaksjF\n/drx7PwUZi5I5YuNmdxzYU9uPbcrYSHfP+4oKStn1a4c5m/NZv6WbDZmHCbI4MEr+nPzqC6B+SNE\nmhANzpJ6tftgAb/7aCNfbsqke3w0v79qAJ1aRbFgW0XIL0k5QH5RKSFBxrDOLRnbO55vduXw5aYs\nbj23K7/5QV+CNfuneIzm3pFG79+bM3now42kHXfqpkNsJGN7xzOmZzzn9og7dlvHsnLHHz/ZyEuL\nd3JRv7b833VnERWmL6niHQp9aRIKS8p4Y0Ua5Q7G9oqne3w0Zqc/in958Q5+//FG+rdvwfM3JdGu\nRUQ9VisSOPUa+mb2IjAByHLODahc1gp4C+gC7ASmOOeq7Jqh0Je6+nJjJve8uZpm4SE8d1MSZ3WM\nDXRJIn5X33PvvAxcesKyB4CvnHM9ga8qH4v43fh+bZlz5yjCQ4OYMnMp761OD3RJIo1KlaHvnFsA\nHDxh8VXA7MrfZwMTfVyXyGn1adecD+4azdBOsdz31rc8/OkmyjT4S6RaattPv61zLqPy931AWx/V\nI1ItraLDeHXaCG4c2ZmZC1L5j5dXcqigONBliTR4dR6c5SouCpz2MMvMpptZspklZ2dn13V3IseE\nBgfxh4kDePjqgSxLOcCVf1/Mxr2HA12WSINW29DPNLMEgMp/s063onPuOedcknMuKT4+vpa7Ezm9\n64d34s3bR1JUWsbVzyzmgzV7Al2SSINV29D/ELi58vebgQ98U45I7Qzt1JKPfjqagR1acO+ba3Se\nX+Q0qgx9M3sDWAr0NrN0M5sG/Bm4yMy2AeMrH4sEVJuYCF7/8Uh+NLITMxekctsryeQVlgS6LJEG\nRYOzpEl6delOHvpoI93jo5l109l0iosKdEkitaZ75IpU4cZzuvDqrcPJPFzEVU8vYknK/kCXJNIg\nKPSlyRrVozUf3HUuraLDuGHWch79fDMlZeWBLkskoBT60qR1aR3Nh3ePZsqwjjw9L4XJzyzRLRzF\n0xT60uRFh4fwyORBPPujoew6WMDl/7eQN1ekUZ/Xs0QaCoW+eMalAxL47N4xDO0cywNz1nHbK8lk\n5xUFuiyReqXQF09p1yKCV28dwW8n9GPBtv1c8tcFfLY+o+oNRZoIhb54TlCQMW10Vz756Wg6xEby\nk9e+4b631uioXzxBoS+e1bNtDHPuHMU9F/Tgo2/3cv5jXzNzfgpFpWWBLk3EbxT64mmhwUH8/OLe\nfH7fGIZ3bcXD/9rMRU8s4LP1+3ShV5okhb4I0D2+GS/ecjazbx1OeEgQP3ltFZNmLGHB1myFvzQp\nCn2R44ztFc+/7j2Ph68eSNbhQm56cQVTZi7ViF5pMjT3jshpFJWW8fbK3fx93nYyDxcxqnscv7y0\nD4N1X16pZ/V6Y3RfUuhLY1RYUsbry9N4et52Dh4p5vKB7bj/4t50j28W6NLEIxT6IgGQX1TKrIWp\nPL8glcLScn4wMIHrh3diZLdWmFmgy5MmTKEvEkD784t49usU3kreTV5hKV1bR3Pd2R25emgi8THh\ngS5PGrBvdx9idVoOt5zbtUbbKfRFGoCjxWV8ui6DN1emsXJnDkEGZ3dpxWUD2nHpgATatYgIdInS\nwEyasZjVaYdY+ZvxNTpAUOiLNDDbs/L48NsMPlufwdbMfACGd2nFz8b3ZFSP1gGuThqC9XtymfDU\nIgCemDKYq4cmVntb3URFpIHp0SaGn1/Ui7n3jeWr+8fyn5f0Jj2ngKmzlnPTiytYvyc30CVKHZWU\nlfPFxkzKa3nv5deX7yIiNIiWUaHM35rt4+qqLyRgexZporrHN+Ou83swbXRXXl26i6e/3s6EpxYx\nYVACVw/twKjurYkIDQ50mVJD/1iexoMfbuAPEwdw48jONdr2cGEJ76/ey5WD21Na5pi3JYuyckdw\n0MkdAHILStiSmcf2rHy2Z+WzLSvPV38CoNAX8ZuI0GBuG9ONKWd3ZOb8FGYv2cnHazMIDwliVPc4\nzu/Tho4to2gWEUJMRAjNwkNo3yKSoFMEgQSWc47ZS3cC8MTcLVwxKIHYqLBqbz9nVTpHS8q4cWQX\nUvfnM2f1HtbtyeWsE8Z8bMvMY+LTizlSXDH/U2RoMN3bRPvqzwAU+iJ+1yIylP+6tA/3ju/J8tSD\n/HtzFv/enMW8LRtOWrd/++b8adJADQBrYBZvP0Bq9hFuH9ON5xem8tcvt/HQlf2rta1zjleX7WJw\nx1gGJragQ8tIzGD+luyTQn/20p2UlDteuDmJ3u1ijh0E2D2++1sU+iL1JDwkmDG94hnTK54Hr+hH\nes5R9ucXkV9USl5hKftyC3l2fgoTZyzmxpGd+cUlvWkeERrospuM1Wk59E1oXqtTa7OX7iQuOoyf\nX9yLI8WlvLpsF1NHdKJX25gqt12aeoCU7CM8OnkQAK2iwxiUGMv8rVncO77nsfXyi0p575s9TBiU\nwIV929a4xurShVyRADAzOraKYkinlpzXM57LByZw6+iufHX/WG4+pwuvLdvFhY/PZ/aSnew6oHv6\n1tXcDfuYNGMJD3148rerquw+WMBXmzK5bnhHwkOCuf+i3jQLD+H3H22s1mR8ry9Lo0VkKFcMbn9s\n2dhe8azZfYhDBcXHln24Zi9Hisu4YUTNrhfUlEJfpAGJiQjloSv78/5d55LQIoIHP9zA2Ee/5ry/\n/JtfzVnLB2v2sOvAEc38WQP7cgv5r3fXEmTw7jfp7MstrNH2ry9PAzgWxi2jw7hvfE8Wbd/PFxsz\nz7ht5uFCPt+wjylJid/7hjGudzzlDhZuq5jIzznHa8t20addDEM7+ffUnk7viDRAgxJj+eCuc0nd\nf4TF2/ezaNt+Pl6bwRsrdgMQGxXKoMRYBie2oE+75vRuF0OXuChCgnUcd7yycsd9b62hqKScF285\nm2mzk5m1MJX/ntCvWtsXlpTx1so0Lu7XjvaxkceW/2hkZ/6xIo0/frKJsb3jCQ859SmjN1akUVru\nmHrC0fvgxFhaRFZ03bxicHvW7D7ExozD/HHiAL9P6aHQF2mgzIzu8c3oHt+Mm87pQmlZOZv35bE2\nPZe16YdYs/sQT8/L5rtu42EhQfRq24xxvdpw+cAE+ibEeH5OoJkLUliaeoBHrhnIuN5tuHJwe/6x\nIo27zu9By+jv977JLyqloLiUNjH/fyT1R9/uJaeghJtGfT+0Q4KD+J8J/fnRC8v51bvrePTawSd1\nv1y4LZsZ81IY37cNXVt/vwdOcJBxXs/WzK+8X8Nry9KIDgtm4pAOPm6Bkyn0RRqJkOAgBnRowYAO\nLZg6ohNQcSS6PSufLfvy2JKZx7e7DzHj6+38fd52uraO5vKB7ejTrjlx0WG0ahZGXHQ4cdFhnugW\numb3IZ6Yu5UfDExgSlJHAO4Y1533Vu9h9tKd/Gx8r2Pr5haUMGXmUrZl5XFh37bcdE5nzu3emtlL\nd9KzTTPO6RZ30uuP7tmaX1zci8fmbsXM+MvkQceCf9WuHKa/sopu8dE8fu1Zp6xvXO82fLw2g6Wp\nB/h47V4mD0ukWbj/I7lOezCznUAeUAaU+mqYsIhUT0Ro8LEPgu/szy/i8w37+HRdBs98ncKJA0hb\nRoUyslsc53SPY1T3OLrERZNXWMqhoyUcKiim3FV0HW1IA8jKyx2FpWWEhwSfckDTdwqKS9myL49N\nGXk8Oz+Fts0j+NOkgce+8fRqG8P4vm15afFObjuvG9HhIRSWlHHbq8mk7s/n+uGd+Gz9Pr7YmEmH\n2Ej2HDrKH85wyuXuC3pS7uCJL7YC8JfJg9iWlcd/vLSCNs3DeWXacFpEnboH1pieFdNzPPDuOopK\ny/1+Afc7vvhYOd85p9sKiTQQrZuFc8OIztwwojO5R0vYl1vIgSNFHDxSzIH8YtbtyWVpygH+tX7f\naV8jNNgY0KEFZ3dpRb+E5mTkFrItM49tWfnsPHCE/u2bc8Xg9lw2IIFW0dUbpOSc41BBCWkHCzh4\npJg+CTEktIiscrslKfv59Zx17DxQcKy28JBgwkOCCPvuJziI4rJy0g4W8N017lbRYTx347CTQvfO\n87tz9YxM3liRxn+c25X73lrDih0H+dv1Q7hycHv+54p+fLZ+H68s3YUZTKrilMs9F/bEOXjyy60U\nlpSxYudBIsOCeW3aiO+dKjpRm+YR9EtozsaMwwzpFEu/9s2rbAtfqNOEa5VH+knVDX1NuCbSMDjn\n2H3wKEtT97P3UCGxUaEVP5FhlJSV803aIZJ3HmRtei7FZeUAtGseQc+2zUhsGcnyHQdJzT5CcJBV\njC7u3YYhnWLp374FYSEVF5Nzj5awNGU/C7ft59v0Q+w6UEBeYen36mjXPIKhnWMZ0rElI7q1on/7\nFseO5A8XlvDwp5t5Y0UaneOimJLUkdKyiiP+opJyCkvLKCktp7isnJKycsyMXm1i6JsQQ9+E5iS2\njDztEfp1zy1lx/4jjO/blteXp/HbCf2YNrpm0x2f6K9fbuWvX24jNiqUt28/p1p9+P/y2WZmfJ3C\n49cO5pphp5+ArcHMsmlmO4AcwAEznXPPnWl9hb5I41JYUsbOA0doHxv5vYFizjk2ZeTx8dq9fLIu\ng12VR+FhIUEMaN+ccgdr0w9R7iA6LJihnVvStXU0nVpF0alVFLFRYWzYm8vqtEOs3p3D7oNHAYiJ\nCGFE1zgGdmjBGyvSyMor5MfndeO+8b2IDPPd6aaF27K58YUVAEwf041fX97XJ6/7ydoMerVtRs9q\nBD5UjAF4YdEOHriszxlPpzWk0O/gnNtjZm2AL4CfOucWnLDOdGA6QKdOnYbt2rWrLvWKSAO0L7eQ\nb9JyWJ2WwzdphwA4t3sco3vGM6RTLKFVdCXNOlzI0tQDLEs9wNKUA+w8UEDvtjE8MnnQSVMV+IJz\njttfXUWb5uH8/soBDf7CdoMJ/e+9kNlDQL5z7rHTraMjfRGpjgP5RbSIDNW4g0oNYj59M4s2s5jv\nfgcuBtb7oigR8ba4ZuEKfD+pS++dtsB7lRdKQoB/OOc+80lVIiLiF7UOfedcKjDYh7WIiIif6fuT\niIiHKPRFRDxEoS8i4iEKfRERD1Hoi4h4iEJfRMRDFPoiIh6i0BcR8RCFvoiIhyj0RUQ8RKEvIuIh\nCn0REQ9R6IuIeIhCX0TEQxT6IiIeotAXEfEQhb6IiIco9EVEPEShLyLiIQp9EREPUeiLiHiIQl9E\nxEMU+iIiHqLQFxHxEIW+iIiHKPRFRDxEoS8i4iEKfRERD6lT6JvZpWa2xcy2m9kDvipKRET8o9ah\nb2bBwNPAZUA/4Hoz6+erwkRExPfqcqQ/HNjunEt1zhUDbwJX+aYsERHxh5A6bNsB2H3c43RgxIkr\nmdl0YHrlwyIzW1+HfVZHCyDXz9tWtd6Znj/dc6dafuKyEx+3BvafsdK6a4ztWZtl9dGWp6vD19vV\ntj313qzdevXRnr2rqKH6nHO1+gEmA7OOe3wj8Pcqtkmu7f5qUNdz/t62qvXO9PzpnjvV8hOXneKx\n2rMa7VadZfXRlnVpz5psV9v21Huzdus1tvasy+mdPUDH4x4nVi4LtI/qYduq1jvT86d77lTLT1xW\nl7+tthpje9Zlmb/Vdp812a627an3Zu3Wa1TtaZWfIjXf0CwE2ApcSEXYrwSmOuc2nGGbZOdcUq12\nKCdRe/qO2tK31J6+5cv2rPUtQdwAAAACxklEQVQ5fedcqZndDXwOBAMvninwKz1X2/3JKak9fUdt\n6VtqT9/yWXvW+khfREQaH43IFRHxEIW+iIiHKPRFRDykwYS+mXUys/fN7EXN41M3ZnaemT1rZrPM\nbEmg62nszCzIzP7XzJ4ys5sDXU9jZ2bjzGxh5Xt0XKDraezMLNrMks1sQnXW90noVwZ11omjbWs4\nIdtA4B3n3K3AEF/U1Rj5oi2dcwudcz8BPgZm+7Pehs5H782rqBiHUkLFyHPP8lF7OiAfiMDD7emj\ntgT4JfB2tffri947ZjaGiv/EV5xzAyqXBVPRj/8iKv5jVwLXU9G98+ETXuJWoAx4h4o3xKvOuZfq\nXFgj5Iu2dM5lVW73NjDNOZdXT+U3OD56b94K5DjnZprZO865yfVVf0Pjo/bc75wrN7O2wBPOuRvq\nq/6GxEdtORiIo+IDdL9z7uOq9luXuXeOcc4tMLMuJyw+NiEbgJm9CVzlnHsYOOlriJn9Aniw8rXe\nATwZ+r5oy8p1OgG5Xg588Nl7Mx0ornxY5r9qGz5fvT8r5QDh/qizMfDRe3McEE3FTMdHzexT51z5\nmfbrk9A/jWpNyHacz4CHzGwqsNOPdTVGNW1LgGl49IOzGmrannOAp8zsPGCBPwtrpGrUnmZ2NXAJ\nEAv83b+lNTo1akvn3G8AzOwWKr9BVbUDf4Z+jTjn1lMxiZv4gHPuwUDX0FQ45wqo+BAVH3DOzaHi\ng1R8xDn3cnXX9WfvnYY6IVtjpLb0LbWnb6k9fcfvbenP0F8J9DSzrmYWBlwHfOjH/TVlakvfUnv6\nltrTd/zelr7qsvkGsBTobWbpZjbNOVcKfDch2ybg7WpMyOZ5akvfUnv6ltrTdwLVlppwTUTEQxrM\niFwREfE/hb6IiIco9EVEPEShLyLiIQp9EREPUeiLiHiIQl9ExEMU+iIiHqLQFxHxkP8HMbplT490\nVFsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uh-97bpLZCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(51)\n",
        "np.random.seed(51)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9),metrics=[\"mae\"])\n",
        "history = model.fit(dataset,epochs=500,verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icGDaND7z0ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forecast = []\n",
        "results = []\n",
        "for time in range(len(series) - window_size):\n",
        "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
        "\n",
        "forecast = forecast[split_time-window_size:]\n",
        "results = np.array(forecast)[:, 0, 0]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plot_series(time_valid, x_valid)\n",
        "plot_series(time_valid, results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfPeqI7rz4LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUsdZB_tzDLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "mae=history.history['mae']\n",
        "loss=history.history['loss']\n",
        "\n",
        "epochs=range(len(loss)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, mae, 'r')\n",
        "plt.plot(epochs, loss, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "epochs_zoom = epochs[200:]\n",
        "mae_zoom = mae[200:]\n",
        "loss_zoom = loss[200:]\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot Zoomed MAE and Loss\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs_zoom, mae_zoom, 'r')\n",
        "plt.plot(epochs_zoom, loss_zoom, 'b')\n",
        "plt.title('MAE and Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"MAE\", \"Loss\"])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CGaYFxXNEAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
        "model.fit(dataset,epochs=100, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ3R8ysauz9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                      input_shape=[None]),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 100.0)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n",
        "model.fit(dataset,epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}